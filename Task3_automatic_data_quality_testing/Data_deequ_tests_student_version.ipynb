{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Big Data - Deequ Analysis\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "## Honour Code\n",
    "I **PRECIOUS**, **AROVO**, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "    Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "Having completed manual data quality checks, it should be obvious that the process can become quite cumbersome. As the Data Engineer in the team, you have researched some tools that could potentially save the team from having to do this cumbersome work. In your research, you have come a across a tool called [Deequ](https://github.com/awslabs/deequ), which is a library for measuring the data quality of large datasets.\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/raw/master/data_engineering/transform/predict/DataQuality.jpg\"\n",
    "     alt=\"Data Quality\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=100%/>\n",
    "     <p><em>Figure 1. Six dimensions of data quality</em></p>\n",
    "</div>\n",
    "\n",
    "You present this tool to your manager; he is quite impressed and gives you the go-ahead to use this in your implementation. You are now required to perform some data quality tests using this automated data testing tool.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 🚩️ Important Notice 🚩️\n",
    ">\n",
    ">To successfully run `pydeequ` without any errors, please make sure that you have an environment that is running pyspark version 3.0.\n",
    "> You are advised to **create a new conda environment** and install this specific version of pyspark to avoid any technical issues:\n",
    ">\n",
    "> `pip install pyspark==3.0`\n",
    "\n",
    "<br>\n",
    "\n",
    "## Import dependencies\n",
    "\n",
    "If you do not have `pydeequ` already installed, install it using the following command:\n",
    "- `pip install pydeequ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.0\"\n",
    "#os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15908\\1849336197.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pydeequ\n",
    "import json\n",
    "import sagemaker_pyspark\n",
    "\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.profiles import *\n",
    "from pydeequ.suggestions import *\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DecimalType, DoubleType, IntegerType, FloatType, DateType, NumericType, StructType, StringType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into spark dataframe\n",
    "\n",
    "In this notebook, we set out to run some data quality tests, with the possiblity of running end to end on the years 1963, 1974, 1985, 1996, 2007, and 2018. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Data_ingestion_student_version.ipynb` notebook to create the parquet files for the following years:\n",
    ">       - 1963\n",
    ">       - 1974\n",
    ">       - 1985\n",
    ">       - 1996\n",
    ">       - 2007\n",
    ">       - 2018\n",
    ">\n",
    ">2. Ingest the data for the for the years given above. You should only do it one year at a time.\n",
    ">3. Ingest the metadata file.\n",
    "\n",
    "\n",
    "When developing your code, it will be sufficient to focus on a single year. However, after your development is done, you will need to run this notebook for all of the given years above so that you can answer all the questions given in the Data Testing MCQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting the data created in data_ingestion_student_version.ipynb notebook, for the different year specified.\n",
    "stock_data_path = r\"C:\\Users\\USER\\Desktop\\ExploreDE\\processing-big-data-predict\\Task1_data_ingestion\\1963_stock_data\"\n",
    "stock_data = spark.read.parquet(stock_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5020"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nasdaq Traded</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security Name</th>\n",
       "      <th>Listing Exchange</th>\n",
       "      <th>Market Category</th>\n",
       "      <th>ETF</th>\n",
       "      <th>Round Lot Size</th>\n",
       "      <th>Test Issue</th>\n",
       "      <th>Financial Status</th>\n",
       "      <th>CQS Symbol</th>\n",
       "      <th>NASDAQ Symbol</th>\n",
       "      <th>NextShares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation Common Stock</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>AAAU</td>\n",
       "      <td>Perth Mint Physical Gold ETF</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAAU</td>\n",
       "      <td>AAAU</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>AACG</td>\n",
       "      <td>ATA Creativity Global - American Depositary Sh...</td>\n",
       "      <td>Q</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AACG</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>AADR</td>\n",
       "      <td>AdvisorShares Dorsey Wright ADR ETF</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AADR</td>\n",
       "      <td>AADR</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nasdaq Traded Symbol                                      Security Name  \\\n",
       "0             Y      A            Agilent Technologies, Inc. Common Stock   \n",
       "1             Y     AA                    Alcoa Corporation Common Stock    \n",
       "2             Y   AAAU                       Perth Mint Physical Gold ETF   \n",
       "3             Y   AACG  ATA Creativity Global - American Depositary Sh...   \n",
       "4             Y   AADR                AdvisorShares Dorsey Wright ADR ETF   \n",
       "\n",
       "  Listing Exchange Market Category ETF  Round Lot Size Test Issue  \\\n",
       "0                N                   N           100.0          N   \n",
       "1                N                   N           100.0          N   \n",
       "2                P                   Y           100.0          N   \n",
       "3                Q               G   N           100.0          N   \n",
       "4                P                   Y           100.0          N   \n",
       "\n",
       "  Financial Status CQS Symbol NASDAQ Symbol NextShares  \n",
       "0              NaN          A             A          N  \n",
       "1              NaN         AA            AA          N  \n",
       "2              NaN       AAAU          AAAU          N  \n",
       "3                N        NaN          AACG          N  \n",
       "4              NaN       AADR          AADR          N  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the metadata \"symbols_valid_meta.csv\" into pandas data frame.\n",
    "metadata_path = r\"C:\\Users\\USER\\Desktop\\ExploreDE\\processing-big-data-predict\\symbols_valid_meta.csv\"\n",
    "\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run tests on the dataset**\n",
    "\n",
    "## Test 1 - Null values ⛔️\n",
    "For the first test, you are required to check the data for completeness.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check for missing values in the data. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the display options to prevent truncating for when using .toPandas() to display result\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Disable column width restriction\n",
    "pd.set_option('display.max_colwidth', None)  # Disable column content width restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stock']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet data\n",
    "df = stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(date,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(open,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(high,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(low,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(close,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(adj_close,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(volume,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(stock,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     check check_level check_status  \\\n",
       "0  Data Completeness Check       Error      Success   \n",
       "1  Data Completeness Check       Error      Success   \n",
       "2  Data Completeness Check       Error      Success   \n",
       "3  Data Completeness Check       Error      Success   \n",
       "4  Data Completeness Check       Error      Success   \n",
       "5  Data Completeness Check       Error      Success   \n",
       "6  Data Completeness Check       Error      Success   \n",
       "7  Data Completeness Check       Error      Success   \n",
       "\n",
       "                                             constraint constraint_status  \\\n",
       "0       CompletenessConstraint(Completeness(date,None))           Success   \n",
       "1       CompletenessConstraint(Completeness(open,None))           Success   \n",
       "2       CompletenessConstraint(Completeness(high,None))           Success   \n",
       "3        CompletenessConstraint(Completeness(low,None))           Success   \n",
       "4      CompletenessConstraint(Completeness(close,None))           Success   \n",
       "5  CompletenessConstraint(Completeness(adj_close,None))           Success   \n",
       "6     CompletenessConstraint(Completeness(volume,None))           Success   \n",
       "7      CompletenessConstraint(Completeness(stock,None))           Success   \n",
       "\n",
       "  constraint_message  \n",
       "0                     \n",
       "1                     \n",
       "2                     \n",
       "3                     \n",
       "4                     \n",
       "5                     \n",
       "6                     \n",
       "7                     "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking completeness of the stock data\n",
    "\n",
    "# Seting up PyDeequ for Completeness Check\n",
    "check = Check(spark, CheckLevel.Error, \"Data Completeness Check\")\n",
    "\n",
    "# Looping through the columns of the stock data\n",
    "for column in df.columns:\n",
    "    checkResult = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check.isComplete(column)\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "resultDataFrame = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "resultDataFrame.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Zero Values 🅾️\n",
    "\n",
    "For the second test, you are required to check for zero values within the dataset.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check for zero values within the data. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify numerical columns\n",
    "numerical_cols = ['open', 'high', 'low', 'close', 'adj_close', 'volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(open,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(high,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.06785380095243454 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(low,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.06563635170459747 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(close,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.06607984006404877 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(adj_close,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 4.89296041905618E-7 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(volume,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               check check_level check_status  \\\n",
       "0  Zero Values Check       Error        Error   \n",
       "1  Zero Values Check       Error        Error   \n",
       "2  Zero Values Check       Error        Error   \n",
       "3  Zero Values Check       Error        Error   \n",
       "4  Zero Values Check       Error        Error   \n",
       "5  Zero Values Check       Error        Error   \n",
       "\n",
       "                                   constraint constraint_status  \\\n",
       "0       MinimumConstraint(Minimum(open,None))           Success   \n",
       "1       MinimumConstraint(Minimum(high,None))           Failure   \n",
       "2        MinimumConstraint(Minimum(low,None))           Failure   \n",
       "3      MinimumConstraint(Minimum(close,None))           Failure   \n",
       "4  MinimumConstraint(Minimum(adj_close,None))           Failure   \n",
       "5     MinimumConstraint(Minimum(volume,None))           Success   \n",
       "\n",
       "                                                     constraint_message  \n",
       "0                                                                        \n",
       "1  Value: 0.06785380095243454 does not meet the constraint requirement!  \n",
       "2  Value: 0.06563635170459747 does not meet the constraint requirement!  \n",
       "3  Value: 0.06607984006404877 does not meet the constraint requirement!  \n",
       "4  Value: 4.89296041905618E-7 does not meet the constraint requirement!  \n",
       "5                                                                        "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for zero values in the dataset\n",
    "\n",
    "# Setting up PyDeequ for Zero Values Check\n",
    "check_zero = Check(spark, CheckLevel.Error, \"Zero Values Check\")\n",
    "\n",
    "# Looping through the numerical columns of the dataset\n",
    "for column in numerical_cols:\n",
    "    checkResult_zero = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check_zero.hasMin(column, lambda x: x == 0)\n",
    "        ) \\\n",
    "        .run()\n",
    "\n",
    "# Displaying the results\n",
    "resultDataFrame_zero = VerificationResult.checkResultsAsDataFrame(spark, checkResult_zero)\n",
    "resultDataFrame_zero.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Negative values ➖️\n",
    "The third test requires you to check that all values in the data are positive.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check negative values within the dataset. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(open is non-negative,COALESCE(CAST(open AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(high is non-negative,COALESCE(CAST(high AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(low is non-negative,COALESCE(CAST(low AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(close is non-negative,COALESCE(CAST(close AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(adj_close is non-negative,COALESCE(CAST(adj_close AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(volume is non-negative,COALESCE(CAST(volume AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   check check_level check_status  \\\n",
       "0  Negative Values Check       Error      Success   \n",
       "1  Negative Values Check       Error      Success   \n",
       "2  Negative Values Check       Error      Success   \n",
       "3  Negative Values Check       Error      Success   \n",
       "4  Negative Values Check       Error      Success   \n",
       "5  Negative Values Check       Error      Success   \n",
       "\n",
       "                                                                                                               constraint  \\\n",
       "0            ComplianceConstraint(Compliance(open is non-negative,COALESCE(CAST(open AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "1            ComplianceConstraint(Compliance(high is non-negative,COALESCE(CAST(high AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "2              ComplianceConstraint(Compliance(low is non-negative,COALESCE(CAST(low AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "3          ComplianceConstraint(Compliance(close is non-negative,COALESCE(CAST(close AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "4  ComplianceConstraint(Compliance(adj_close is non-negative,COALESCE(CAST(adj_close AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "5        ComplianceConstraint(Compliance(volume is non-negative,COALESCE(CAST(volume AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "\n",
       "  constraint_status constraint_message  \n",
       "0           Success                     \n",
       "1           Success                     \n",
       "2           Success                     \n",
       "3           Success                     \n",
       "4           Success                     \n",
       "5           Success                     "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for zero values in the dataset\n",
    "\n",
    "# Setting up PyDeequ for Negative Values Check\n",
    "constraints = Check(spark, CheckLevel.Error, \"Negative Values Check\")\n",
    "\n",
    "# Looping through the numerical columns of the dataset\n",
    "for column in numerical_cols:\n",
    "    checkResult_zero = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            constraints.isNonNegative(column)\n",
    "        ) \\\n",
    "        .run()\n",
    "\n",
    "# Displaying the results\n",
    "resultDataFrame_zero = VerificationResult.checkResultsAsDataFrame(spark, checkResult_zero)\n",
    "resultDataFrame_zero.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4 - Determine Maximum Values ⚠️\n",
    "\n",
    "For the fourth test, we want to find the maximum values in the dataset for the numerical fields. Extremum values can often be used to define an upper bound for the column values so we can define them as the threshold values. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Column Profiler Runner` to generate summary statistics for all the available columns. \n",
    ">2. Extract the maximum values for all the numeric columns in the data.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# numerical_cols = ['open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "\n",
    "# Setting up PyDeequ for Determining Maximum values\n",
    "result = ColumnProfilerRunner(spark) \\\n",
    "    .onData(df) \\\n",
    "    .run()\n",
    "\n",
    "# Extracting and printing the maximum values for the specified numerical columns\n",
    "for column in numerical_cols:\n",
    "    column_profile = result.profiles[column]\n",
    "    max_value = column_profile.histogram.max\n",
    "    print(f\"Maximum value for {column}: {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+-----------------+------------------+-----------------+\n",
      "|summary|              open|             high|               low|            close|         adj_close|           volume|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+-----------------+\n",
      "|  count|              5020|             5020|              5020|             5020|              5020|             5020|\n",
      "|   mean|1.1365598366658882|19.17383405059962|18.943191553180316|19.06198000288253|  7.20845818042487|525223.0677290837|\n",
      "| stddev| 4.757480282400392|61.95001257800808| 61.21457985181483|61.60101770760706|29.681165595553537|910433.9606092008|\n",
      "|    min|               0.0|        0.0678538|        0.06563635|       0.06607984|      4.8929604E-7|              0.0|\n",
      "|    25%|               0.0|       0.30110678|         0.2985026|       0.30110678|       0.005313047|          42400.0|\n",
      "|    50%|               0.0|        2.3945312|         2.3632812|         2.390625|        0.12447533|         188800.0|\n",
      "|    75%|        0.72365785|         6.908625|           6.81651|          6.86457|         0.7633495|         748800.0|\n",
      "|    max|           303.125|          315.625|           311.875|           313.75|         148.77046|        2.06928E7|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+-----------------+\n",
      "\n",
      "Maximum value for open: 303.125\n",
      "Maximum value for high: 315.625\n",
      "Maximum value for low: 311.875\n",
      "Maximum value for close: 313.75\n",
      "Maximum value for adj_close: 148.7704620361328\n",
      "Maximum value for volume: 20692800.0\n"
     ]
    }
   ],
   "source": [
    "# Generating a pyspark automated process to provide the maximum values for each column as in df as the ColumnProfilerRunner\n",
    "# seem not to be working.\n",
    "\n",
    "# Generate summary statistics for all available columns\n",
    "summary_statistics = df.select(numerical_cols).summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\")\n",
    "\n",
    "# Showing summary statistics for a general overview\n",
    "summary_statistics.show()\n",
    "\n",
    "# Extracting and display maximum values for all the numeric columns\n",
    "for column in numerical_cols:\n",
    "    max_value = df.agg(max(column).alias(f'max_{column}')).collect()[0][f'max_{column}']\n",
    "    print(f\"Maximum value for {column}: {max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5 - Stock Tickers 💹️\n",
    "\n",
    "For the fifth test, we want to determine if the stock tickers contained in our dataset are consistent. To do this, you will need to make use of use of the metadata file to check that the stock names used in the dataframe are valid. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to determine if the stock tickers contained in the dataset appear in the metadata file.\n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the metadata to \n",
    "meta = spark.createDataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_stock = df.groupBy('stock').count()\n",
    "distinct_symbol = metadata['Symbol'].unique()\n",
    "\n",
    "# Converting both data to list\n",
    "stock_column = distinct_stock.toPandas()\n",
    "stock_column = stock_column.values.tolist()\n",
    "symbol_column = distinct_symbol.tolist()\n",
    "\n",
    "stock_symbol = [item[0] for item in stock_column]\n",
    "\n",
    "for stock in stock_symbol:\n",
    "    if stock not in symbol_column:\n",
    "        print(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Stock Ticker Veri...|      Error|     Success|ComplianceConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "# Converting the meta DataFrame's 'Symbol' column to a Python list of allowed values\n",
    "allowed_stock_symbols = meta.select(\"Symbol\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Useing VerificationSuite from PyDeequ to verify the stock tickers in the DataFrame `df`\n",
    "verificationResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        Check(spark, CheckLevel.Error, \"Stock Ticker Verification\") \\\n",
    "            .isContainedIn(\"stock\", allowed_stock_symbols, \n",
    "                           hint=\"The stock ticker is not listed in the metadata.\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Displaying the results of the verification\n",
    "VerificationResult.checkResultsAsDataFrame(spark, verificationResult).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6 - Duplication 👥️\n",
    "Lastly, we want to determine the uniqueness of the items found in the dataframe. You need to make use of the Verification Suite to check for the validity of the stock tickers. \n",
    "\n",
    "Similar to the previous notebook - `Data_profiling_student_version.ipynb`, the first thing to check will be if the primary key values within the dataset are unique - in our case, that will be a combination of the stock name and the date. Secondly, we want to check if the entries are all unique, which is done by checking for duplicates across that whole dataset.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to determine the uniqueness of entries contained within the dataset.\n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplication check results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Duplication Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(Stream(stock, ?),None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               check check_level check_status  \\\n",
       "0  Duplication Check     Warning      Success   \n",
       "\n",
       "                                                constraint constraint_status  \\\n",
       "0  UniquenessConstraint(Uniqueness(Stream(stock, ?),None))           Success   \n",
       "\n",
       "  constraint_message  \n",
       "0                     "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the primary key for uniqueness verification\n",
    "primary_key = ['stock', 'date']\n",
    "\n",
    "# Initializing VerificationSuite with the DataFrame\n",
    "verification_suite = VerificationSuite(spark).onData(df)\n",
    "\n",
    "# Defining a check for uniqueness\n",
    "duplication_check = Check(spark, CheckLevel.Warning, \"Duplication Check\")\\\n",
    "    .hasUniqueness(primary_key, lambda x: x == 1) # Corrected variable name\n",
    "\n",
    "# Adding the check to the VerificationSuite\n",
    "verification_result = verification_suite.addCheck(duplication_check).run()\n",
    "\n",
    "# Displaying the results\n",
    "print(\"Duplication check results:\")\n",
    "results_df = VerificationResult.checkResultsAsDataFrame(spark, verification_result)\n",
    "results_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b41f59b882618484a4d28c089dca4efdf4ffb1e043e654ec6730d7439b802f5"
  },
  "kernelspec": {
   "display_name": "predict",
   "language": "python",
   "name": "predict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
